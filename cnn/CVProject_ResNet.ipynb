{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CVProject_ResNet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "eNqJmaFsRhlS",
        "colab_type": "code",
        "outputId": "d9f9caef-62d8-4cc3-f37d-1d7796ae27f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "# Install pytorch and tqdm (if necessary)\n",
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install tqdm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (0.4.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.3.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (0.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wA4n2ZsKP-dt",
        "colab_type": "code",
        "outputId": "f84c345b-1cea-4dbb-e402-9bd4fe065077",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Mount your google drive as the data drive\n",
        "# This will require google authorization\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V3Hu3-dhIaga",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Handle imports\n",
        "\n",
        "import math\n",
        "import os\n",
        "import datetime\n",
        "import csv\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import torch.nn.init as init\n",
        "#import torchvision.models as models\n",
        "\n",
        "from IPython import display"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z7cF6oVvIesG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The Args object will contain all of our parameters\n",
        "# If you want to run with different arguments, create another Args object\n",
        "\n",
        "class Args(object):\n",
        "  def __init__(self, name='mnist', batch_size=64, test_batch_size=1000,\n",
        "            epochs=10, lr=0.0001,optimizer='adam', momentum=0.5,\n",
        "            seed=1, log_interval=100, dataset='mnist',\n",
        "            data_dir='/content/drive/My Drive/cs482/data', model='default',\n",
        "            cuda=True):\n",
        "    self.name = name # name for this training run. Don't use spaces.\n",
        "    self.batch_size = batch_size\n",
        "    self.test_batch_size = test_batch_size # Input batch size for testing\n",
        "    self.epochs = epochs # Number of epochs to train\n",
        "    self.lr = lr # Learning rate\n",
        "    self.optimizer = optimizer # sgd/p1sgd/adam/rmsprop\n",
        "    self.momentum = momentum # SGD Momentum\n",
        "    self.seed = seed # Random seed\n",
        "    self.log_interval = log_interval # Batches to wait before logging\n",
        "                                     # detailed status. 0 = never\n",
        "    self.dataset = dataset # mnist/fashion_mnist\n",
        "    self.data_dir = data_dir\n",
        "    self.model = model # default/P2Q7DoubleChannelsNet/P2Q7HalfChannelsNet/\n",
        "                  # P2Q8BatchNormNet/P2Q9DropoutNet/P2Q10DropoutBatchnormNet/\n",
        "                  # P2Q11ExtraConvNet/P2Q12RemoveLayerNet/P2Q13UltimateNet\n",
        "    self.cuda = cuda and torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ft6KtNDRvO1m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "\n",
        "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
        "           'resnet152']\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "}\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "       ##modification 1, using dropout with residual branch\n",
        "        x = F.dropout(x, p=0.1, training=self.training)\n",
        "        ##modification 1 ends  https://medium.com/@14prakash/understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-the-art-image-cc5d0adf648e\n",
        "        \n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = conv1x1(inplanes, planes)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = conv3x3(planes, planes, stride)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = conv1x1(planes, planes * self.expansion)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.inplanes = 64\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def resnet18(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet34(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-34 model.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet50(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-50 model.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet101(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-101 model.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet152(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-152 model.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MR-67qrJGCYc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conv_bn_leru(in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "    return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "    )\n",
        "\n",
        "def down_pooling():\n",
        "    return nn.MaxPool2d(2)\n",
        "\n",
        "def up_pooling(in_channels, out_channels, kernel_size=2, stride=2):\n",
        "    return nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "# UNet class\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, input_channels=1, nclasses=10):\n",
        "        super().__init__()\n",
        "        # go down\n",
        "        self.conv1 = conv_bn_leru(input_channels,64)\n",
        "        self.conv2 = conv_bn_leru(64, 128)\n",
        "        self.conv3 = conv_bn_leru(128, 256)\n",
        "        self.conv4 = conv_bn_leru(256, 512)\n",
        "        self.conv5 = conv_bn_leru(512, 1024)\n",
        "        self.down_pooling = nn.MaxPool2d(2)\n",
        "\n",
        "        # go up\n",
        "        self.up_pool6 = up_pooling(1024, 512)\n",
        "        self.conv6 = conv_bn_leru(1024, 512)\n",
        "        self.up_pool7 = up_pooling(512, 256,kernel_size=3)\n",
        "        self.conv7 = conv_bn_leru(512, 256)\n",
        "        self.up_pool8 = up_pooling(256, 128)\n",
        "        self.conv8 = conv_bn_leru(256, 128)\n",
        "        self.up_pool9 = up_pooling(128, 64)\n",
        "        self.conv9 = conv_bn_leru(128, 64)\n",
        "\n",
        "        self.conv10 = nn.Conv2d(64, nclasses, 1)\n",
        "\n",
        "\n",
        "        # test weight init\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal(m.weight.data, a=0, mode='fan_out')\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize input data\n",
        "        x = x/255.\n",
        "        # go down\n",
        "        x1 = self.conv1(x)\n",
        "        p1 = self.down_pooling(x1)\n",
        "        x2 = self.conv2(p1)\n",
        "        p2 = self.down_pooling(x2)\n",
        "        x3 = self.conv3(p2)\n",
        "        p3 = self.down_pooling(x3)\n",
        "        x4 = self.conv4(p3)\n",
        "        #p4 = self.down_pooling(x4)\n",
        "        #x5 = self.conv5(p4)\n",
        "\n",
        "        # go up\n",
        "        #p6 = self.up_pool6(x5)\n",
        "        #print('\\nshapes:',p6.shape,x4.shape,x5.shape)\n",
        "        #x6 = torch.cat([p6, x4], dim=1)\n",
        "        #x6 = self.conv6(x6)\n",
        "\n",
        "        p7 = self.up_pool7(x4)\n",
        "        #print('\\nshapes:',p7.shape,x3.shape,x4.shape)\n",
        "        x7 = torch.cat([p7, x3], dim=1)\n",
        "        x7 = self.conv7(x7)\n",
        "\n",
        "        p8 = self.up_pool8(x7)\n",
        "        x8 = torch.cat([p8, x2], dim=1)\n",
        "        x8 = self.conv8(x8)\n",
        "\n",
        "        p9 = self.up_pool9(x8)\n",
        "        x9 = torch.cat([p9, x1], dim=1)\n",
        "        x9 = self.conv9(x9)\n",
        "\n",
        "\n",
        "        output = self.conv10(x9)\n",
        "        output = F.sigmoid(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "class UNet2(nn.Module):\n",
        "    def __init__(self, input_channels=1, nclasses=10):\n",
        "        super().__init__()\n",
        "        # go down\n",
        "        self.conv1 = conv_bn_leru(input_channels,16)\n",
        "        self.conv2 = conv_bn_leru(16, 32)\n",
        "        self.conv3 = conv_bn_leru(32, 64)\n",
        "        self.conv4 = conv_bn_leru(64, 128)\n",
        "        self.conv5 = conv_bn_leru(128, 256)\n",
        "        self.down_pooling = nn.MaxPool2d(2)\n",
        "\n",
        "        # go up\n",
        "        self.up_pool6 = up_pooling(256, 128)\n",
        "        self.conv6 = conv_bn_leru(256, 128)\n",
        "        self.up_pool7 = up_pooling(128, 64)\n",
        "        self.conv7 = conv_bn_leru(128, 64)\n",
        "        self.up_pool8 = up_pooling(64, 32)\n",
        "        self.conv8 = conv_bn_leru(64, 32)\n",
        "        self.up_pool9 = up_pooling(32, 16)\n",
        "        self.conv9 = conv_bn_leru(32, 16)\n",
        "\n",
        "        self.conv10 = nn.Conv2d(16, nclasses, 1)\n",
        "\n",
        "\n",
        "        # test weight init\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal(m.weight.data, a=0, mode='fan_out')\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize input data\n",
        "        x = x/255.\n",
        "        # go down\n",
        "        x1 = self.conv1(x)\n",
        "        p1 = self.down_pooling(x1)\n",
        "        x2 = self.conv2(p1)\n",
        "        p2 = self.down_pooling(x2)\n",
        "        x3 = self.conv3(p2)\n",
        "        p3 = self.down_pooling(x3)\n",
        "        x4 = self.conv4(p3)\n",
        "        p4 = self.down_pooling(x4)\n",
        "        x5 = self.conv5(p4)\n",
        "\n",
        "        # go up\n",
        "        p6 = self.up_pool6(x5)\n",
        "        x6 = torch.cat([p6, x4], dim=1)\n",
        "        x6 = self.conv6(x6)\n",
        "\n",
        "        p7 = self.up_pool7(x6)\n",
        "        x7 = torch.cat([p7, x3], dim=1)\n",
        "        x7 = self.conv7(x7)\n",
        "\n",
        "        p8 = self.up_pool8(x7)\n",
        "        x8 = torch.cat([p8, x2], dim=1)\n",
        "        x8 = self.conv8(x8)\n",
        "\n",
        "        p9 = self.up_pool9(x8)\n",
        "        x9 = torch.cat([p9, x1], dim=1)\n",
        "        x9 = self.conv9(x9)\n",
        "\n",
        "\n",
        "        output = self.conv10(x9)\n",
        "        output = F.sigmoid(output)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l2Ejw36nIpRr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def prepare_dataset(args):\n",
        "    # choose the dataset\n",
        "    if args.dataset == 'mnist':\n",
        "        DatasetClass = datasets.MNIST\n",
        "    elif args.dataset == 'fashion_mnist':\n",
        "        DatasetClass = datasets.FashionMNIST\n",
        "    else:\n",
        "        raise ValueError('unknown dataset: ' + args.dataset +\n",
        "                ' try mnist or fashion_mnist')\n",
        "\n",
        "    def time_stamp(fname, fmt='%m-%d-%H-%M_{fname}'):\n",
        "        return datetime.datetime.now().strftime(fmt).format(fname=fname)\n",
        "        \n",
        "    training_run_name = time_stamp(args.dataset + '_' + args.name)\n",
        "\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
        "\n",
        "    # Create the dataset: mnist or fasion_mnist\n",
        "    dataset_dir = os.path.join(args.data_dir, args.dataset)\n",
        "    training_run_dir = os.path.join(args.data_dir, training_run_name)\n",
        "    train_dataset = DatasetClass(\n",
        "        dataset_dir, train=True, download=True,\n",
        "        transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ]))\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "    test_dataset = DatasetClass(\n",
        "        dataset_dir, train=False, transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ]))\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset, batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
        "    \n",
        "    if not os.path.exists(training_run_dir):\n",
        "        os.makedirs(training_run_dir)\n",
        "\n",
        "    return train_loader, test_loader, train_dataset, test_dataset, training_run_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AVGaeczNItaS",
        "colab_type": "code",
        "outputId": "ec54b902-ca4c-46f5-fa16-8ecdba76e2e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        }
      },
      "cell_type": "code",
      "source": [
        "# visualize some images\n",
        "\n",
        "args = Args()\n",
        "_, _, _, test_dataset, _ = prepare_dataset(args)\n",
        "images = test_dataset.test_data[:6]\n",
        "labels = test_dataset.test_labels[:6]\n",
        "fig, axes = plt.subplots(1,6)\n",
        "for axis, img, lbl in zip(axes, images, labels):\n",
        "    axis.imshow(img)\n",
        "    axis.set_title(lbl)\n",
        "    axis.set_yticklabels([])\n",
        "    axis.set_xticklabels([])\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAABNCAYAAAAFKbeYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMIxfaFJEswAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fe3949660f0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "tD-StYJ2I6u2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, train_loader, epoch, total_minibatch_count,\n",
        "        train_losses, train_accs):\n",
        "    # Training for a full epoch\n",
        "\n",
        "    model.train()\n",
        "    correct_count, total_loss, total_acc = 0., 0., 0.\n",
        "    progress_bar = tqdm.tqdm(train_loader, desc='Training')\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(progress_bar):\n",
        "        if args.cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        data, target = Variable(data), Variable(target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward prediction step\n",
        "        output = model(data)\n",
        "        computeloss=nn.CrossEntropyLoss()\n",
        "        #computeloss=nn.BCELoss()\n",
        "        loss = computeloss(output, target)\n",
        "        #loss = computeloss(output, data)\n",
        "        \n",
        "        # Backpropagation step\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # The batch has ended, determine the accuracy of the predicted outputs\n",
        "        pred = output.data.max(1)[1]  \n",
        "\n",
        "        # target labels and predictions are categorical values from 0 to 9.\n",
        "        matches = target == pred\n",
        "        accuracy = matches.float().mean()\n",
        "        correct_count += matches.sum()\n",
        "\n",
        "        if args.log_interval != 0 and \\\n",
        "                total_minibatch_count % args.log_interval == 0:\n",
        "\n",
        "            train_losses.append(loss.data[0])\n",
        "            train_accs.append(accuracy.data[0])\n",
        "            \n",
        "        total_loss += loss.data\n",
        "        total_acc += accuracy.data\n",
        "            \n",
        "        progress_bar.set_description(\n",
        "            'Epoch: {} loss: {:.4f}, acc: {:.2f}'.format(\n",
        "                epoch, total_loss / (batch_idx + 1), total_acc / (batch_idx + 1)))\n",
        "        #progress_bar.refresh()\n",
        "\n",
        "        total_minibatch_count += 1\n",
        "\n",
        "    return total_minibatch_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KgCLKEHcI-bg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(model, test_loader, epoch, total_minibatch_count,\n",
        "        val_losses, val_accs):\n",
        "    # Validation Testing\n",
        "    model.eval()\n",
        "    test_loss, correct = 0., 0.\n",
        "    progress_bar = tqdm.tqdm(test_loader, desc='Validation')\n",
        "    with torch.no_grad():\n",
        "        for data, target in progress_bar:\n",
        "            if args.cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            data, target = Variable(data), Variable(target)\n",
        "            output = model(data)\n",
        "            computeloss=nn.CrossEntropyLoss()\n",
        "            #computeloss=nn.BCELoss()\n",
        "            test_loss += computeloss(output, target).data  # sum up batch loss\n",
        "            #test_loss += computeloss(output, data).data  # sum up batch loss\n",
        "            pred = output.data.max(1)[1]  # get the index of the max log-probability\n",
        "            correct += (target == pred).float().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    \n",
        "    acc = correct / len(test_loader.dataset)\n",
        "\n",
        "    val_losses.append(test_loss)\n",
        "    val_accs.append(acc)\n",
        "    \n",
        "    progress_bar.clear()\n",
        "    progress_bar.write(\n",
        "        '\\nEpoch: {} validation test results - Average val_loss: {:.4f}, val_acc: {}/{} ({:.2f}%)'.format(\n",
        "            epoch, test_loss, correct, len(test_loader.dataset),\n",
        "            100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "    return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "khV-_vFSJGT8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Run the experiment\n",
        "def run_experiment(args,fine_tuning):\n",
        "\n",
        "    total_minibatch_count = 0\n",
        "\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.cuda:\n",
        "        torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "    train_loader, test_loader, _, _, run_path = prepare_dataset(args)\n",
        "    #print(\"\\n\\nDataset:  \", args.dataset,  \" Epochs:  \" , args.epochs, \"  lr:  \", args.lr ,\"\\n\")\n",
        "    epochs_to_run = args.epochs\n",
        "\n",
        "    # Choose model\n",
        "    # TODO add all the other models here if their parameter is specified\n",
        "    if  args.model=='ResNet18':\n",
        "        model=resnet18()\n",
        "    elif args.model=='ResNet34':\n",
        "        model=resnet34()\n",
        "    elif args.model=='ResNet50':\n",
        "        model=resnet50()\n",
        "    elif args.model=='ResNet101':\n",
        "        model=resnet101()\n",
        "    elif args.model=='UNet':\n",
        "        model=UNet()\n",
        "    elif args.model in globals():\n",
        "        model = globals()[args.model]()\n",
        "    #elif args.model == 'ProjectResnetPretrained':\n",
        "     #   model = ProjectResnetPretrained()\n",
        "    else:\n",
        "        raise ValueError('Unknown model type: ' + args.model)\n",
        "\n",
        "    if args.cuda:\n",
        "        model.cuda()\n",
        "\n",
        "    # Choose optimizer\n",
        "    if args.optimizer == 'sgd':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
        "    elif args.optimizer == 'adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=args.lr)  ## Added lr here to pass from down there\n",
        "    elif args.optimizer == 'rmsprop':\n",
        "        optimizer = optim.RMSprop(model.parameters())\n",
        "    else:\n",
        "        raise ValueError('Unsupported optimizer: ' + args.optimizer)\n",
        "\n",
        "    # Run the primary training loop, starting with validation accuracy of 0\n",
        "    val_acc = 0\n",
        "    train_losses, train_accs = [], []\n",
        "    val_losses, val_accs = [], []\n",
        "    if fine_tuning==1:\n",
        "      path='/content/drive/My Drive/cs482/data/Q13_best_accuracy_checkpoints/' ## change the path to where thw checkpoint file is\n",
        "      path=os.path.join(path + 'Ultimate_fashion_MNIST_20epochs.pkl') \n",
        "      cuda = torch.cuda.is_available()\n",
        "      if cuda:\n",
        "        checkpoint = torch.load(path)\n",
        "      else:\n",
        "          # Load GPU model on CPU\n",
        "          checkpoint = torch.load(path,\n",
        "                                  map_location=lambda storage,\n",
        "                                  loc: storage)\n",
        "      model.load_state_dict(checkpoint['state_dict'])\n",
        "    for epoch in range(1, epochs_to_run + 1):\n",
        "        \n",
        "        # train for 1 epoch\n",
        "               \n",
        "        total_minibatch_count = train(model, optimizer, train_loader,\n",
        "                                    epoch, total_minibatch_count,\n",
        "                                    train_losses, train_accs)\n",
        "        # validate progress on test dataset\n",
        "        val_acc = test(model, test_loader, epoch, total_minibatch_count,\n",
        "                       val_losses, val_accs)\n",
        "        \n",
        "    fig, axes = plt.subplots(1,4, figsize=(13,4))\n",
        "    # plot the losses and acc\n",
        "    plt.title(args.name)\n",
        "    axes[0].plot(train_losses)\n",
        "    axes[0].set_title(\"Loss\")\n",
        "    axes[1].plot(train_accs)\n",
        "    axes[1].set_title(\"Acc\")\n",
        "    axes[2].plot(val_losses)\n",
        "    axes[2].set_title(\"Val loss\")\n",
        "    axes[3].plot(val_accs)\n",
        "    axes[3].set_title(\"Val Acc\")\n",
        "    \n",
        "    # Write to csv file\n",
        "    with open(os.path.join(run_path + 'train.csv'), 'w') as f:\n",
        "        csvw = csv.writer(f, delimiter=',')\n",
        "        for loss, acc in zip(train_losses, train_accs):\n",
        "            csvw.writerow((loss, acc))\n",
        "\n",
        "    # Predict and Test\n",
        "    images, labels = next(iter(test_loader))\n",
        "    if args.cuda:\n",
        "        images, labels = images.cuda(), labels.cuda()\n",
        "    output = model(images)\n",
        "    predicted = torch.max(output, 1)[1]\n",
        "    fig, axes = plt.subplots(1,6,figsize=(6,2))\n",
        "    for i, (axis, img, lbl) in enumerate(zip(axes, images, predicted)):\n",
        "        if i > 5:\n",
        "            break\n",
        "        img = img.permute(1,2,0).squeeze()\n",
        "        axis.imshow(img)\n",
        "        axis.set_title(lbl.data)\n",
        "        axis.set_yticklabels([])\n",
        "        axis.set_xticklabels([])\n",
        "     \n",
        "    if args.dataset == 'fashion_mnist' and val_acc > 0.92 and val_acc <= 1.0:\n",
        "        print(\"Congratulations, you beat the Question 13 minimum of 92\"\n",
        "            \"with ({:.2f}%) validation accuracy!\".format(val_acc))\n",
        "        path='/content/drive/My Drive/cs482/data/Q13_best_accuracy_checkpoints/'       ##change the path where you wanna save the checkpoint file\n",
        "        state={\n",
        "        'epoch': epoch,\n",
        "        'test_loader': test_loader,\n",
        "        'total_minibatch_count': total_minibatch_count,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'best_accuracy': val_acc,\n",
        "        'args': args\n",
        "        \n",
        "    }\n",
        "        torch.save(state, os.path.join(path + 'fashion_mnist_ResNet_1.pkl'))   ### for saving Ultimate_fashion_MNIST\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OQ6ztrNWJKmm",
        "colab_type": "code",
        "outputId": "7e4ec05a-2bb3-489e-8940-fdd7cd16f723",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "cell_type": "code",
      "source": [
        "run_experiment(Args(name = 'fashion_mnist', dataset='fashion_mnist', batch_size=8, epochs=20, lr=0.0001, optimizer='adam', model='resnet50'),fine_tuning=0) \n",
        "##fine_tuning shall be set to one when checking MNIST on fashion_MNIST\n",
        "## change above as needed"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rTraining:   0%|          | 0/7500 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "Epoch: 1 loss: 0.9467, acc: 0.66: 100%|██████████| 7500/7500 [20:47<00:00,  6.09it/s]\n",
            "Validation: 100%|██████████| 10/10 [00:03<00:00,  2.99it/s]\n",
            "Training:   0%|          | 0/7500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 1 validation test results - Average val_loss: 0.0006, val_acc: 7969.0/10000 (79.69%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 2 loss: 0.5242, acc: 0.81: 100%|██████████| 7500/7500 [21:03<00:00,  6.09it/s]\n",
            "Validation: 100%|██████████| 10/10 [00:03<00:00,  2.98it/s]\n",
            "Training:   0%|          | 0/7500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 2 validation test results - Average val_loss: 0.0004, val_acc: 8537.0/10000 (85.37%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 3 loss: 0.4028, acc: 0.86: 100%|██████████| 7500/7500 [20:41<00:00,  6.06it/s]\n",
            "Validation: 100%|██████████| 10/10 [00:03<00:00,  3.00it/s]\n",
            "Training:   0%|          | 0/7500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 3 validation test results - Average val_loss: 0.0004, val_acc: 8691.0/10000 (86.91%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 4 loss: 0.3021, acc: 0.89:   1%|▏         | 107/7500 [00:18<20:20,  6.06it/s]"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}